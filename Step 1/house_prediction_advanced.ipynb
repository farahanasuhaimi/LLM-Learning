{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced House Price Prediction\n",
    "\n",
    "This notebook explores more advanced techniques for the House Price Prediction task. We will dive deeper into:\n",
    "\n",
    "1.  **Exploratory Data Analysis (EDA)** with Seaborn to find patterns.\n",
    "2.  **Advanced Feature Engineering**, including Target Encoding for categorical features.\n",
    "3.  Training a more robust regression model like **XGBoost** or **Random Forest**.\n",
    "4.  **Model Evaluation** and interpreting feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Data Loading\n",
    "\n",
    "First, we import all the necessary libraries and load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
      "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
      "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
      "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
      "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
      "\n",
      "  YrSold  SaleType  SaleCondition  SalePrice  \n",
      "0   2008        WD         Normal     208500  \n",
      "1   2007        WD         Normal     181500  \n",
      "2   2008        WD         Normal     223500  \n",
      "3   2006        WD        Abnorml     140000  \n",
      "4   2008        WD         Normal     250000  \n",
      "\n",
      "[5 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis (EDA) with Seaborn\n",
    "\n",
    "Let's visualize the data to understand relationships and distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Target Variable Distribution\n",
    "\n",
    "We'll check the distribution of `SalePrice`. Since many models assume a normal distribution, we might need to transform it (e.g., using a log transform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Create a histogram and a Q-Q plot for SalePrice\n",
    "# sns.histplot(df['SalePrice'], kde=True)\n",
    "# plt.title('Distribution of SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Correlation Heatmap\n",
    "\n",
    "A heatmap is a great way to see which numerical features are most correlated with `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Calculate the correlation matrix and plot a heatmap\n",
    "# corr_matrix = df.corr()\n",
    "# plt.figure(figsize=(12, 9))\n",
    "# sns.heatmap(corr_matrix, vmax=.8, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Categorical Feature Analysis\n",
    "\n",
    "Let's see how different categories relate to `SalePrice` using boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Create a boxplot for 'Neighborhood' vs 'SalePrice'\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.boxplot(x='Neighborhood', y='SalePrice', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "This is where we clean the data, create new features, and handle categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Handling Missing Values\n",
    "\n",
    "Identify columns with missing values and decide on a strategy to fill them (e.g., mean, median, or a constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Identify and fill missing values\n",
    "# For simplicity, we'll focus on a few features and drop rows with NaNs in them for now.\n",
    "# Later, you can implement more sophisticated imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Target Encoding\n",
    "\n",
    "Here, we'll implement Target Encoding for a high-cardinality feature like `Neighborhood`. To prevent data leakage, we'll calculate the encoding on the training set and apply it to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select features and target\n",
    "features = ['Neighborhood', 'LotArea', 'GrLivArea', 'TotalBsmtSF', 'YearBuilt']\n",
    "target = 'SalePrice'\n",
    "df_subset = df[features + [target]].dropna()\n",
    "\n",
    "# 2. Split data BEFORE encoding\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_subset[features], df_subset[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Calculate target encoding on the training set\n",
    "# Your code here: Calculate the mean SalePrice for each Neighborhood in the training data\n",
    "# neighborhood_map = X_train.groupby('Neighborhood')['SalePrice'].mean() ... (this is conceptual, you need to group y_train by X_train's neighborhood)\n",
    "\n",
    "# 4. Apply the encoding to both training and validation sets\n",
    "# X_train['Neighborhood_encoded'] = X_train['Neighborhood'].map(neighborhood_map)\n",
    "# X_val['Neighborhood_encoded'] = X_val['Neighborhood'].map(neighborhood_map)\n",
    "\n",
    "# 5. Fill any potential NaNs in validation set (if a neighborhood was not in the training set)\n",
    "# X_val['Neighborhood_encoded'].fillna(y_train.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Now we'll train a more powerful model. A Random Forest is a great choice as it's robust and handles non-linear relationships well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Prepare the final feature set (dropping original categorical columns)\n",
    "# X_train_final = X_train.drop('Neighborhood', axis=1)\n",
    "# X_val_final = X_val.drop('Neighborhood', axis=1)\n",
    "\n",
    "# Initialize and train the model\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, random_state=42, oob_score=True)\n",
    "# rf_model.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Performance Metrics\n",
    "\n",
    "Let's see how our model performed on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Make predictions and calculate RMSE and R-squared\n",
    "# predictions = rf_model.predict(X_val_final)\n",
    "# rmse = np.sqrt(mean_squared_error(y_val, predictions))\n",
    "# r2 = r2_score(y_val, predictions)\n",
    "# print(f'RMSE: {rmse}')\n",
    "# print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Feature Importance\n",
    "\n",
    "Let's find out which features the model found most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Get feature importances from the trained model and plot them\n",
    "# importances = rf_model.feature_importances_\n",
    "# feature_names = X_train_final.columns\n",
    "# importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "# importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# sns.barplot(x='Importance', y='Feature', data=importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Next Steps\n",
    "\n",
    "Summarize your findings here. What worked well? What could be improved?\n",
    "\n",
    "**Next Steps:**\n",
    "- Implement more sophisticated feature engineering (e.g., creating `HouseAge` from `YearBuilt`).\n",
    "- Try other models like XGBoost or LightGBM.\n",
    "- Perform hyperparameter tuning to optimize the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
